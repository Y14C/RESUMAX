"""
LM Studio API Handler for Resumax Application

This module handles all interactions with LM Studio's locally hosted API for resume formatting.
LM Studio uses an OpenAI-compatible API structure.

PARAMETERS RECEIVED FROM main.py:
    - api_key: str - API key for LM Studio authentication (can be any string for local)
    - model_name: str - Selected LM Studio model identifier
    - system_prompt: str - System prompt read from Model_API/system-prompt.txt
    - latex_format: str - LaTeX template content read from Latex_formats/ATS.tex
    - extracted_text: str - Plain text content extracted from uploaded resume
    - base_url: str (optional) - LM Studio API endpoint (default: http://localhost:1234/v1)

RETURNS TO main.py:
    - latex_code: str - Formatted LaTeX resume code generated by LM Studio

ERROR HANDLING:
    - All API errors, network issues, connection failures are raised as exceptions
    - No fallback logic provided - errors propagate to main.py for handling

NOTE:
    - LM Studio must be running locally with a model loaded
    - Models are fetched dynamically from the running LM Studio instance
"""

from openai import OpenAI
import logging
from typing import List

# Configure logging for this module
logger = logging.getLogger(__name__)

# API Configuration
TEMPERATURE = 0.4
DEFAULT_BASE_URL = "http://localhost:1234/v1"


def get_available_models(api_key: str = "lm-studio", base_url: str = DEFAULT_BASE_URL) -> List[str]:
    """
    Returns list of available models from running LM Studio instance.
    
    CALLED BY: main.py during model selection phase
    RETURNS TO: main.py for display in UI
    
    PARAMETERS:
        - api_key: API key for authentication (default: "lm-studio", can be any string)
        - base_url: LM Studio API endpoint (default: http://localhost:1234/v1)
    
    RAISES:
        - Exception: If LM Studio is not running or connection fails
    """
    try:
        # Initialize client to connect to local LM Studio with single retry
        client = OpenAI(
            api_key=api_key, 
            base_url=base_url,
            max_retries=1,  # Only retry once
            timeout=5.0     # 5 second timeout
        )
        
        # Fetch available models from LM Studio
        models = client.models.list()
        
        # Extract model IDs
        model_list = [model.id for model in models.data]
        
        if not model_list:
            raise Exception(
                "No models found in LM Studio. "
                "Please ensure LM Studio is running and a model is loaded."
            )
        
        return model_list
        
    except Exception as e:
        raise Exception(
            f"Failed to connect to LM Studio at {base_url}. "
            f"Ensure LM Studio is running with a model loaded. Error: {str(e)}"
        ) from e


def format_resume(
    api_key: str,
    model_name: str,
    system_prompt: str,
    latex_format: str,
    extracted_text: str,
    base_url: str = DEFAULT_BASE_URL
) -> str:
    """
    Main function to format resume using LM Studio API.
    
    CALLED BY: main.py after user configuration and file upload
    
    RECEIVES FROM main.py:
        - api_key: API key for LM Studio (can be any string for local use)
        - model_name: Selected model from get_available_models() or user input
        - system_prompt: Content from Model_API/system-prompt.txt
        - latex_format: Content from Latex_formats/ATS.tex
        - extracted_text: Processed text from upload_handler.py/pdf_handler.py
        - base_url: LM Studio API endpoint (default: http://localhost:1234/v1)
    
    RETURNS TO main.py:
        - Formatted LaTeX resume code as string
    
    RAISES:
        - ValueError: If model name is missing or blank
        - Exception: For API errors, connection issues, or unexpected errors
    """
    # Log function entry
    logger.info(f"[AI REQUEST] LM Studio format_resume() called with model: {model_name}, base_url: {base_url}")
    
    # Validate model name
    if not model_name or not model_name.strip():
        raise ValueError("Model name is required for LM Studio")
    
    # Initialize OpenAI client to communicate with LM Studio local server with single retry
    client = OpenAI(
        api_key=api_key, 
        base_url=base_url,
        max_retries=1,  # Only retry once
        timeout=90.0    # 90 second timeout for AI operations
    )

    # Prepare combined user prompt (template + resume text)
    user_message = _build_user_message(latex_format, extracted_text)

    try:
        logger.info(f"[AI REQUEST] Making LM Studio API call to {model_name} with temperature {TEMPERATURE}")
        response = client.chat.completions.create(
            model=model_name,
            temperature=TEMPERATURE,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
        )

        latex_code = _extract_latex_from_response(response)
        logger.info(f"[AI RESPONSE] LM Studio API call successful - Response length: {len(latex_code)} characters")
        return latex_code

    except Exception as e:
        error_type = type(e).__name__

        if "AuthenticationError" in error_type:
            logger.error(f"[AI ERROR] LM Studio Authentication Error for {model_name}: {str(e)}")
            raise Exception(f"Authentication Error: Invalid API key - {str(e)}") from e
        elif "RateLimitError" in error_type:
            logger.error(f"[AI ERROR] LM Studio Rate Limit Error for {model_name}: {str(e)}")
            raise Exception(f"Rate Limit Error: {str(e)}") from e
        elif "APIError" in error_type:
            logger.error(f"[AI ERROR] LM Studio API Error for {model_name}: {str(e)}")
            raise Exception(f"LM Studio API Error: {str(e)}") from e
        else:
            logger.error(f"[AI ERROR] Unexpected error in LM Studio API call for {model_name}: {str(e)}")
            raise Exception(f"Unexpected error in LM Studio API call: {str(e)}") from e


def _build_user_message(latex_format: str, extracted_text: str) -> str:
    """
    Constructs the user message combining LaTeX format template and extracted resume text.
    Internal function - not called from outside this module.
    """
    return f"""Here is the LaTeX format template to use:

{latex_format}

---

Here is the resume content to format:

{extracted_text}

---

Please format the above resume content using the provided LaTeX template. Return only the complete LaTeX code, ready to compile."""


def _extract_latex_from_response(response) -> str:
    """
    Extracts LaTeX code from LM Studio's API response.
    Internal function - not called from outside this module.
    """
    # LM Studio returns OpenAI-compatible response structure
    if not response.choices:
        raise Exception("Empty response received from LM Studio API")
    
    # Extract text from first choice
    latex_code = response.choices[0].message.content
    
    if not latex_code or len(latex_code.strip()) == 0:
        raise Exception("No LaTeX code found in LM Studio's response")
    
    return latex_code
